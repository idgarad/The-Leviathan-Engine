# Ceph RADOS Gateway (RGW) Setup Guide (Homelab, Proxmox Host)

> **Language Notice:** As of October 2025, Rust is the unified backend language for all simulation, orchestration, and infrastructure code in The Leviathan Engine project. All future code examples and integrations will use Rust.

> **Update Notice:** Accurate as of October 2025. For latest info, see official Ceph documentation.
> 
> **For additional information, please see:**
> - Ceph RGW Docs: https://docs.ceph.com/en/latest/radosgw/
> - [Book] "Learning Ceph" by Anthony D’Atri
> - [YouTube] Search "Ceph S3 setup homelab" for recent guides

---

## Step 1: Plan Your Ceph Deployment

**Tool Explanation:**
Ceph is an open-source, distributed storage platform supporting block, file, and object storage. RADOS Gateway (RGW) provides S3-compatible object storage on top of Ceph, ideal for Terraform state, backups, and scalable homelab storage.

**Role in Infrastructure:**
Ceph RGW acts as the backend for Terraform state and other persistent data, providing true fault tolerance, scalability, and S3 API compatibility.

**Further Reading & References:**
- [Ceph RGW Overview (YouTube)](https://www.youtube.com/results?search_query=ceph+s3+setup)
- [Book] "Learning Ceph" by Anthony D’Atri

---

## Step 2: Prepare Proxmox Host and Disks

1. Use your Proxmox host directly (no need for separate VMs).
2. Attach one or more physical or virtual disks to the Proxmox host for Ceph OSDs (object storage daemons).
3. Assign a static IP to the Proxmox host for Ceph RGW access.
4. Ensure the host can communicate over the network (firewall rules, etc.).

> **Teaching Note:** Running Ceph directly on the Proxmox host simplifies setup and maximizes performance. For true redundancy, use multiple Proxmox hosts in a cluster. For testing or small homelabs, a single host works but lacks fault tolerance.

---

## Step 3: Install Ceph on the Proxmox Host

1. Update and install Ceph packages:
   ```bash
   sudo apt update
   sudo apt install ceph ceph-common ceph-mgr ceph-mon ceph-osd ceph-radosgw ceph-volume
   ```
2. Initialize the Ceph cluster (on the Proxmox host):
   ```bash
   ceph-deploy new <proxmox-hostname>
   ceph-deploy install <proxmox-hostname>
   ceph-deploy mon create-initial
   ```
3. **Add OSDs (Object Storage Daemons) Using a File-Based Approach**

   **Note:** Proxmox 9 does not allow Ceph to use file-based OSDs by default in the GUI. You must use the command line to manually create a file to use as an OSD. This is a workaround for homelab environments without spare physical disks.

   **Step-by-Step:**
   1. Create a file to act as a virtual disk for the OSD (example: 10GB) in `/opt`:
      ```bash
      sudo dd if=/dev/zero of=/opt/ceph-osd1.img bs=1M count=10240
      ```
   2. Set up a loop device for the file:
      ```bash
      sudo losetup -fP /opt/ceph-osd1.img
      # Find the loop device name
      sudo losetup -a
      # Example output: /dev/loop0: [2065]:4194305 (/opt/ceph-osd1.img)
      ```
   3. Create the OSD using ceph-volume:
      ```bash
      sudo ceph-volume lvm create --data /dev/loop0
      ```
      - This will initialize and add the OSD using the file-backed loop device.
   4. Verify OSDs are added and active:
      ```bash
      ceph osd tree
      ceph osd status
      ceph health
      ```

   > **Teaching Note:** File-based OSDs are not recommended for production, but can be useful for homelab testing. Performance and reliability will be lower than with dedicated disks.

4. Check cluster health:
   ```bash
   ceph health
   ceph status
   ```

---

## Step 4: Deploy RADOS Gateway (RGW)

1. Install RGW on the Proxmox host:
   ```bash
   sudo apt install ceph-radosgw
   ```
2. Create RGW instance:
   ```bash
   ceph-deploy rgw create <proxmox-hostname>
   ```
3. Verify RGW is running and accessible (default port 7480):
   ```bash
   curl http://<proxmox-ip>:7480
   ```

---

## Step 5: Create S3 User and Bucket for Terraform

1. Create a dedicated S3 user for Terraform:
   ```bash
   radosgw-admin user create --uid="terraform" --display-name="Terraform User"
   ```
2. Note the access and secret keys from the output.
3. Create a bucket for Terraform state (using AWS CLI or s3cmd):
   ```bash
   aws --endpoint-url http://<node1-ip>:7480 s3 mb s3://terraform-state
   ```

---

## Step 6: Configure Terraform Backend

Use the RGW endpoint, access key, and secret key in your Terraform `backend` block:
```hcl
terraform {
  backend "s3" {
    endpoint   = "http://<node1-ip>:7480"
    bucket     = "terraform-state"
    key        = "global/terraform.tfstate"
    region     = "us-east-1"
    access_key = "terraform-user"
    secret_key = "your-terraform-password"
    skip_credentials_validation = true
    skip_metadata_api_check     = true
    force_path_style           = true
  }
}
```

---

## Step 7: Backup and Maintenance

- Regularly back up Ceph data directories and configuration.
- Monitor cluster health with `ceph status` and Ceph dashboard.
- Test recovery and failover procedures.

---

## Troubleshooting & Best Practices

- Use at least 3 nodes for production/fault tolerance.
- Keep Ceph updated for security and stability.
- Document all configuration and credentials.
- Use firewall rules to restrict access to RGW ports.

---

> **Update Notice:** Instructions accurate as of October 2025. For latest info, see official docs.
> 
> **Further Reading:**
> - https://docs.ceph.com/en/latest/radosgw/
> - "Learning Ceph" by Anthony D’Atri
